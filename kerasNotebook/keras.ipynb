{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some material\n",
    "   \n",
    "### Is the data shuffled during training?\n",
    "\n",
    "Yes, if the shuffle argument in model.fit is set to True (which is the default), the training data will be randomly shuffled at each epoch.\n",
    "\n",
    "Validation data is never shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten,Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# np.random.seed(2016) \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo\n",
    "0. check whether the gradient decent is not shuffled  ---- done \n",
    "1. make the intilization the same   --- done \n",
    "2. make the datasets clustered  ----done \n",
    "3. randomized the clustered batch but its clusterng\n",
    "\n",
    "\n",
    "# the original complex keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "47616/60000 [======================>.......] - ETA: 8s - loss: 0.3761 - acc: 0.8855"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-924522fad792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a simpler SGD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.6508 - acc: 0.8433 - val_loss: 0.3052 - val_acc: 0.9128\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.2870 - acc: 0.9184 - val_loss: 0.2448 - val_acc: 0.9317\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.2360 - acc: 0.9328 - val_loss: 0.2077 - val_acc: 0.9390\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.2025 - acc: 0.9430 - val_loss: 0.1859 - val_acc: 0.9463\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1789 - acc: 0.9497 - val_loss: 0.1631 - val_acc: 0.9530\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1590 - acc: 0.9551 - val_loss: 0.1594 - val_acc: 0.9542\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1444 - acc: 0.9588 - val_loss: 0.1364 - val_acc: 0.9606\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1321 - acc: 0.9620 - val_loss: 0.1303 - val_acc: 0.9619\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1219 - acc: 0.9653 - val_loss: 0.1215 - val_acc: 0.9645.96 - ETA: 0s - loss: 0.1221 - acc:\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1134 - acc: 0.9671 - val_loss: 0.1228 - val_acc: 0.9642\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1060 - acc: 0.9696 - val_loss: 0.1099 - val_acc: 0.9673\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.0999 - acc: 0.9711 - val_loss: 0.1069 - val_acc: 0.9694\n",
      "Test loss: 0.106879658414\n",
      "Test accuracy: 0.9694\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "17s - loss: 0.7178 - acc: 0.8193 - val_loss: 0.3196 - val_acc: 0.9094\n",
      "Test loss: 0.319608975106\n",
      "Test accuracy: 0.9094\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS SGD TESTING\n",
    "start from here we are testing the keras SGD random or not, we use small sample of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test=x_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test=y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (5000, 28, 28, 1)\n",
      "5000 train samples\n",
      "1000 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "5000/5000 [==============================] - 8s - loss: 1.4065 - acc: 0.6968 - val_loss: 0.7591 - val_acc: 0.8070\n",
      "Test loss: 0.759067568779\n",
      "Test accuracy: 0.807\n",
      "[2.3049223, 2.2985256, 2.2503564, 2.23441, 2.2234449, 2.2072704, 2.1865511, 2.2155533, 2.1858051, 2.1327934, 2.1990898, 2.1380169, 2.1394863, 2.103225, 2.0474067, 2.0465572, 2.0727258, 1.9859893, 2.041285, 1.9573172, 1.9655675, 1.9812416, 1.9547147, 1.9515655, 2.0535624, 1.8622813, 1.8708495, 1.8733219, 1.8495508, 1.8177255, 1.7837791, 1.8093818, 1.7864228, 1.7551713, 1.6808974, 1.6290058, 1.7298985, 1.6548636, 1.5631753, 1.6652234, 1.4998133, 1.4984107, 1.4360192, 1.4625568, 1.363209, 1.5035322, 1.4647084, 1.3634138, 1.4724407, 1.423772, 1.321414, 1.3188597, 1.214731, 1.2739127, 1.3362553, 1.235743, 1.2167237, 1.040653, 1.1309727, 1.1397777, 1.1723866, 1.103657, 1.0912642, 0.99940914, 0.94918859, 1.0763081, 1.1367857, 1.0637504, 1.0026662, 0.79261637, 0.87819529, 0.83536613, 0.79376453, 0.86479247, 0.86266452, 0.85155499, 0.90317357, 0.88909763, 1.0159655, 0.76894772, 0.85069048, 0.82468623, 0.91659057, 0.71294498, 0.88365251, 0.78887117, 0.75858307, 0.8007558, 0.76175386, 0.66194952, 0.7351917, 0.84791619, 0.80403364, 0.69142652, 0.85679936, 0.81937689, 0.59881306, 0.73300576, 0.82875723, 0.79295349]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    # np.random.seed(2016) \n",
    "    batch_size = 50\n",
    "    epochs = 1\n",
    "\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    batch_history = LossHistory()\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks = [batch_history])\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print(batch_history.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('keras_stochas_test.h5')\n",
    "#Assuming you have code for instantiating your model, you can then load the weights you saved into a model with the same architecture:\n",
    "\n",
    "model.load_weights('keras_stochas_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "5000/5000 [==============================] - 0s - loss: 0.4911 - acc: 0.8752 - val_loss: 0.4847 - val_acc: 0.8570\n",
      "Test loss: 0.484671998024\n",
      "Test accuracy: 0.857\n",
      "[0.74001825, 0.50991201, 0.67236155, 0.63878816, 0.58162868, 0.58588982, 0.49227825, 0.44537508, 0.60324645, 0.61554861, 0.81974787, 0.62652075, 0.81085724, 0.63647437, 0.58922052, 0.50366402, 0.52365553, 0.72443247, 0.74182057, 0.51571625, 0.72931361, 0.80613327, 0.7977227, 0.41920802, 0.66189563, 0.5539059, 0.45504004, 0.69632268, 0.56362122, 0.41638905, 0.54011858, 0.56462443, 0.4719522, 0.50067538, 0.33932644, 0.49572638, 0.49912605, 0.4305236, 0.46893668, 0.36471185, 0.56305152, 0.50469106, 0.29429141, 0.38880646, 0.43068427, 0.3661879, 0.409724, 0.47507751, 0.61898494, 0.3900775, 0.29899412, 0.44382507, 0.41442293, 0.52878785, 0.54420567, 0.48296213, 0.48953086, 0.29178518, 0.50160062, 0.43842167, 0.5282855, 0.58993357, 0.41710049, 0.35561889, 0.41067433, 0.49818552, 0.31359819, 0.58761752, 0.38501012, 0.37142059, 0.61493379, 0.36351019, 0.27638412, 0.55661404, 0.45471004, 0.45846593, 0.41743988, 0.32518482, 0.35878998, 0.342022, 0.28102133, 0.57798684, 0.46101761, 0.57744765, 0.32036161, 0.43766263, 0.48834184, 0.41004321, 0.30174226, 0.45654821, 0.37843686, 0.24324566, 0.47529745, 0.51853216, 0.26317567, 0.53444099, 0.4972057, 0.26363698, 0.43480033, 0.53379714]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_weights('keras_stochas_test.h5')\n",
    "with tf.device('/gpu:1'):\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks = [batch_history],\n",
    "              shuffle = False) # we can already see that the shuffle affects training a lot\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print(batch_history.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "here \n",
    "1. we make keras stop shuffling the data \n",
    "2. we make sure it gives a determinisc GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# make data clustered\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load and formating data first \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30207,  5662, 55366, ..., 23285, 15728, 11924])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_X_train = x_train[y_train.argsort()]\n",
    "sorted_Y_train = y_train[y_train.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(sorted_Y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5421\n"
     ]
    }
   ],
   "source": [
    "num_each = min(counts)\n",
    "print (num_each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we want a list of indices of each number \n",
    "\n",
    "# array of indices of 0\n",
    "\n",
    "sorted_Y_train[range(0,counts[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind1 = np.argwhere(sorted_Y_train  == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5923],\n",
       "       [ 5924],\n",
       "       [ 5925],\n",
       "       ..., \n",
       "       [12662],\n",
       "       [12663],\n",
       "       [12664]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5923,  5924,  5925, ..., 12662, 12663, 12664])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind1.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listOfIndOfNum = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listOfIndOfNum = [np.argwhere(sorted_Y_train ==i).flatten() for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n",
      "[1 1 1 ..., 1 1 1]\n",
      "[2 2 2 ..., 2 2 2]\n",
      "[3 3 3 ..., 3 3 3]\n",
      "[4 4 4 ..., 4 4 4]\n",
      "[5 5 5 ..., 5 5 5]\n",
      "[6 6 6 ..., 6 6 6]\n",
      "[7 7 7 ..., 7 7 7]\n",
      "[8 8 8 ..., 8 8 8]\n",
      "[9 9 9 ..., 9 9 9]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(sorted_Y_train[listOfIndOfNum[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# now let us make a function that can join them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def join(partitions_inds):\n",
    "    cluster_size = len(partitions_inds)\n",
    "#     print (\"cluster size:\",cluster_size)\n",
    "    cluster_num = min([len(i) for i in partitions_inds])\n",
    "#     print (\"cluster_num:\",cluster_num)\n",
    "    ret = np.arange(cluster_size * cluster_num)\n",
    "#     print (ret)\n",
    "#     print (ret.shape)\n",
    "    for i in range(cluster_num):\n",
    "        for j in range(cluster_size):\n",
    "            pass\n",
    "            ret[cluster_size*i+j] = partitions_inds[j][i]\n",
    "#             print (j,i)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mixed_ind = join(listOfIndOfNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(threshold=100)\n",
    "sorted_Y_train[mixed_ind].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustered_Y_train = sorted_Y_train[mixed_ind]\n",
    "clustered_X_train = sorted_X_train[mixed_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary\n",
    "I have made the indices that are well clustered\n",
    "\n",
    "notice that the training data here is already formated and can be directly used for model.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare cluster with fixed or shuffled\n",
    "\n",
    "it may be useful \n",
    "\n",
    "np.random.permutation(10)\n",
    "\n",
    "array([1, 7, 4, 3, 0, 9, 2, 5, 8, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 8, 5, 7, 0, 4, 6, 9, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustered_x_train shape: (54210, 28, 28, 1)\n",
      "54210 train samples\n"
     ]
    }
   ],
   "source": [
    "# let us first make Y to be one hot \n",
    "num_classes = 10\n",
    "clustered_y_train = keras.utils.to_categorical(clustered_Y_train, num_classes)\n",
    "clustered_x_train = clustered_X_train\n",
    "\n",
    "# let us normalize a bit the intilization value\n",
    "\n",
    "clustered_x_train = clustered_x_train.astype('float32')\n",
    "clustered_x_train /= 255\n",
    "\n",
    "print('clustered_x_train shape:', clustered_x_train.shape)\n",
    "print(clustered_x_train.shape[0], 'train samples')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let make several random shuffled dataset\n",
    "rnd_inds = [np.random.permutation(clustered_x_train.shape[0]) for i in range(20)]\n",
    "rnd_xs = [clustered_x_train[ind] for ind in rnd_inds]\n",
    "rnd_ys = [clustered_y_train[ind] for ind in rnd_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rnd_xs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-58fd741bcfd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrnd_xs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrnd_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rnd_xs' is not defined"
     ]
    }
   ],
   "source": [
    "del (rnd_xs)\n",
    "del (rnd_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we can load our model here to ensure the same inilization\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = logs.get(\"loss\",[])\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "def makeModel_small(input_shape):\n",
    "    # np.random.seed(2016) \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['acc'])\n",
    "    return model \n",
    "\n",
    "def makeModel_large(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "def makeModel(input_shape):\n",
    "    return makeModel_large(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we make a large model here and save it won't run everytime\n",
    "# model = makeModel_large(input_shape)\n",
    "\n",
    "# model.save(\"complex_cnn_model.h5\")\n",
    "\n",
    "# model.save_weights(\"complex_cnn_model_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6c1577370de2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "[ i.shape for i in model.get_weights()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def train_model(model,x_train,y_train,batch_size=50,epochs=1,weights_file = None):\n",
    "    batch_history = LossHistory()\n",
    "    model.load_weights(weights_file)\n",
    "    model_history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              shuffle = False,\n",
    "              callbacks = [batch_history])\n",
    "#     print(batch_history.losses)\n",
    "    return model_history,batch_history\n",
    "\n",
    "def train_model_large(model,x_train,y_train,batch_size=50,epochs=1):\n",
    "    return train_model(model,x_train,y_train,batch_size=batch_size,\\\n",
    "                       epochs=epochs,weights_file = \"complex_cnn_model_weights.h5\")\n",
    "def train_model_small(model,x_train,y_train,batch_size=50,epochs=1,weights_file = None):\n",
    "    return train_model(model,x_train,y_train,batch_size=batch_size,\\\n",
    "                       epochs=epochs,weights_file = 'keras_stochas_test.h5')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = makeModel_large(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasize = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 4\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3221 - acc: 0.9051    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1044 - acc: 0.9683    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0756 - acc: 0.9769    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0593 - acc: 0.9819    \n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    clustered_model_his,clustered_batch_his = \\\n",
    "        train_model_large(model,clustered_x_train[:datasize],clustered_y_train[:datasize],\\\n",
    "                    epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3279 - acc: 0.9008    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1079 - acc: 0.9675    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0763 - acc: 0.9766    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0607 - acc: 0.9811    \n"
     ]
    }
   ],
   "source": [
    "perm_num = 4\n",
    "with tf.device('/gpu:1'):\n",
    "    normal_model_his,normal_batch_his = \\\n",
    "        train_model_large(model,rnd_xs[perm_num][:datasize],rnd_ys[perm_num][:datasize],\\\n",
    "                    epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3296 - acc: 0.9001    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1068 - acc: 0.9672    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0767 - acc: 0.9771    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0599 - acc: 0.9814    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3327 - acc: 0.9000    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1056 - acc: 0.9673    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0753 - acc: 0.9763    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0586 - acc: 0.9816    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3304 - acc: 0.9010    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1094 - acc: 0.9667    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0775 - acc: 0.9764    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0602 - acc: 0.9815    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3335 - acc: 0.8983    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1090 - acc: 0.9666    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0778 - acc: 0.9771    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0603 - acc: 0.9810    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3332 - acc: 0.8990    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1084 - acc: 0.9667    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0751 - acc: 0.9777    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0593 - acc: 0.9815    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3278 - acc: 0.8999    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1084 - acc: 0.9672    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0774 - acc: 0.9766    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0588 - acc: 0.9816    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3276 - acc: 0.8997    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1062 - acc: 0.9674    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0755 - acc: 0.9768    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0590 - acc: 0.9825    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3339 - acc: 0.8994    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1070 - acc: 0.9676    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0762 - acc: 0.9764    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0599 - acc: 0.9816    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3316 - acc: 0.9010    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1062 - acc: 0.9674    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0753 - acc: 0.9773    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0598 - acc: 0.9814    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3285 - acc: 0.9006    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1073 - acc: 0.9676    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0769 - acc: 0.9770    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0590 - acc: 0.9823    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3334 - acc: 0.8981    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1078 - acc: 0.9673    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0779 - acc: 0.9763    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0607 - acc: 0.9813    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3287 - acc: 0.9004    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1074 - acc: 0.9668    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0758 - acc: 0.9768    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0593 - acc: 0.9819    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3325 - acc: 0.8983    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1064 - acc: 0.9671    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0769 - acc: 0.9763    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0592 - acc: 0.9819    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3341 - acc: 0.8991    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1064 - acc: 0.9676    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0761 - acc: 0.9766    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0579 - acc: 0.9821    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3297 - acc: 0.9001    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1080 - acc: 0.9661    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0771 - acc: 0.9766    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0587 - acc: 0.9816    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3301 - acc: 0.8993    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1066 - acc: 0.9683    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0752 - acc: 0.9768    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0586 - acc: 0.9825    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3351 - acc: 0.8992    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1088 - acc: 0.9672    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0767 - acc: 0.9764    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0593 - acc: 0.9812    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3297 - acc: 0.9009    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1084 - acc: 0.9663    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0772 - acc: 0.9764    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0602 - acc: 0.9816    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3289 - acc: 0.8995    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1080 - acc: 0.9667    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0766 - acc: 0.9758    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0576 - acc: 0.9821    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3290 - acc: 0.9001    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1074 - acc: 0.9673    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0761 - acc: 0.9767    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0602 - acc: 0.9811    \n"
     ]
    }
   ],
   "source": [
    "normal_batch_hises = []\n",
    "for perm_num_num in range(20):\n",
    "    with tf.device('/gpu:1'):\n",
    "        normal_model_his,normal_batch_his = \\\n",
    "            train_model_large(model,rnd_xs[perm_num][:datasize],rnd_ys[perm_num][:datasize],\\\n",
    "                        epochs=epochs,batch_size=batch_size)\n",
    "        normal_batch_hises.append(normal_batch_his)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runningMeanFast(x, N):\n",
    "    return np.convolve(x, np.ones((N,))/N)[(N-1):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJn2llCpC/h/Lnemmi/YrkQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb980320890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "start=400\n",
    "end  = 10000\n",
    "window = 300\n",
    "clus_plot = runningMeanFast(clustered_batch_his.losses[start:end],window)\n",
    "norm_plot = runningMeanFast(normal_batch_his.losses[start:end],window)\n",
    "plt.plot(clus_plot, label = \"clus_loss\")\n",
    "plt.plot(norm_plot, label= \"normal_loss\")\n",
    "plt.legend()\n",
    "plt.savefig('Keras_deep_cmpGradient_epo'+str(epochs)+'_batsiz'+str(batch_size)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.082280375"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(clustered_batch_his.losses[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.090038016"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(normal_batch_his.losses[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clustered_batch_his.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04575412"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([np.mean(i.losses[-10:]) for i in normal_batch_hises])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054629546"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(clustered_batch_his.losses[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save(\"losses_of_20_rand_normal\",[i.losses for i in normal_batch_hises])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_batch_hises = np.load(\"losses_of_20_rand_normal.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3e3c473a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start=400\n",
    "end  = 10000\n",
    "window = 20\n",
    "for i in range(20):\n",
    "    plt.plot(runningMeanFast(normal_batch_hises[i][start:end],window),color = \"red\", label= \"normal_loss \"+str(i))\n",
    "# plt.legend()\n",
    "plt.savefig('Keras_deep_cmpGradient_bunchofnormal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Activation',\n",
       " 'Conv2D',\n",
       " 'Dense',\n",
       " 'Dropout',\n",
       " 'Flatten',\n",
       " 'K',\n",
       " 'LossHistory',\n",
       " 'MaxPooling2D',\n",
       " 'Sequential',\n",
       " 'batch_size',\n",
       " 'clustered_X_train',\n",
       " 'clustered_Y_train',\n",
       " 'clustered_x_train',\n",
       " 'clustered_y_train',\n",
       " 'counts',\n",
       " 'datasize',\n",
       " 'end',\n",
       " 'epochs',\n",
       " 'i',\n",
       " 'img_cols',\n",
       " 'img_rows',\n",
       " 'ind',\n",
       " 'ind1',\n",
       " 'input_shape',\n",
       " 'join',\n",
       " 'keras',\n",
       " 'listOfIndOfNum',\n",
       " 'makeModel',\n",
       " 'makeModel_large',\n",
       " 'makeModel_small',\n",
       " 'matplotlib',\n",
       " 'mixed_ind',\n",
       " 'mnist',\n",
       " 'model',\n",
       " 'normal_batch_hises',\n",
       " 'np',\n",
       " 'num_classes',\n",
       " 'num_each',\n",
       " 'plt',\n",
       " 'print_function',\n",
       " 'rnd_inds',\n",
       " 'rnd_ys',\n",
       " 'runningMeanFast',\n",
       " 'sorted_X_train',\n",
       " 'sorted_Y_train',\n",
       " 'start',\n",
       " 'tf',\n",
       " 'train_model',\n",
       " 'train_model_large',\n",
       " 'train_model_small',\n",
       " 'unique',\n",
       " 'window',\n",
       " 'x_test',\n",
       " 'x_train',\n",
       " 'y_test',\n",
       " 'y_train']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "who_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ?reset_selective -f a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cluster_shuffle(partitions):\n",
    "    num_part = len(partitions)\n",
    "    size_part = min([len(i) for i in partitions])\n",
    "#     print(num_part)\n",
    "#     print(size_part)\n",
    "    shuffled_inds = [ np.random.permutation(size_part) for i in range(num_part)]\n",
    "#     print (shuffled_inds)\n",
    "    return [part[inds] for part,inds in zip(partitions,shuffled_inds)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#let make several random shuffled dataset\n",
    "clustered_rnd_inds = [join(cluster_shuffle(listOfIndOfNum)) for i in range(20)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#they come from raw , so we need some processing\n",
    "# let us make a formated version of the sorted\n",
    "formatted_sorted_X_train = sorted_X_train.astype(\"float32\")\n",
    "formatted_sorted_X_train /=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "num_classes = 10\n",
    "formatted_sorted_Y_train = keras.utils.to_categorical(sorted_Y_train,num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clustered_rnd_xs = [formatted_sorted_X_train[ind] for ind in clustered_rnd_inds]\n",
    "clustered_rnd_ys = [formatted_sorted_Y_train[ind] for ind in clustered_rnd_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3105 - acc: 0.9069    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1015 - acc: 0.9688    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0732 - acc: 0.9776    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0557 - acc: 0.9830    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3129 - acc: 0.9063    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1031 - acc: 0.9690    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0735 - acc: 0.9768    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0572 - acc: 0.9825    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3152 - acc: 0.9056    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1047 - acc: 0.9676    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0746 - acc: 0.9767    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0580 - acc: 0.9818    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3066 - acc: 0.9088    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1036 - acc: 0.9680    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0719 - acc: 0.9782    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0578 - acc: 0.9820    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3178 - acc: 0.9047    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1074 - acc: 0.9671    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0746 - acc: 0.9774    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0586 - acc: 0.9817    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3133 - acc: 0.9066    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1052 - acc: 0.9685    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0741 - acc: 0.9776    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0574 - acc: 0.9821    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3191 - acc: 0.9049    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1075 - acc: 0.9674    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0766 - acc: 0.9763    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0599 - acc: 0.9817    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3093 - acc: 0.9067    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1031 - acc: 0.9691    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0744 - acc: 0.9765    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0576 - acc: 0.9816    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3149 - acc: 0.9057    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1056 - acc: 0.9685    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0765 - acc: 0.9762    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0575 - acc: 0.9819    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3124 - acc: 0.9074    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1023 - acc: 0.9691    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0746 - acc: 0.9765    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 15s - loss: 0.0560 - acc: 0.9825    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 14s - loss: 0.3102 - acc: 0.9059    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 14s - loss: 0.1030 - acc: 0.9688    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 14s - loss: 0.0742 - acc: 0.9770    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 14s - loss: 0.0576 - acc: 0.9821    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3098 - acc: 0.9078    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1046 - acc: 0.9678    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 15s - loss: 0.0738 - acc: 0.9769    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 14s - loss: 0.0563 - acc: 0.9827    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 14s - loss: 0.3108 - acc: 0.9068    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 14s - loss: 0.1048 - acc: 0.9673    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0744 - acc: 0.9773    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0573 - acc: 0.9823    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3093 - acc: 0.9076    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1034 - acc: 0.9683    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0735 - acc: 0.9773    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0580 - acc: 0.9824    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3104 - acc: 0.9062    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1031 - acc: 0.9677    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0747 - acc: 0.9766    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0579 - acc: 0.9817    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3116 - acc: 0.9066    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1044 - acc: 0.9671    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0736 - acc: 0.9772    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0573 - acc: 0.9818    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3126 - acc: 0.9069    - ETA: 1s - loss:\n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1020 - acc: 0.9685    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0733 - acc: 0.9775    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0571 - acc: 0.9825    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3108 - acc: 0.9068    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1033 - acc: 0.9686    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0723 - acc: 0.9785    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0571 - acc: 0.9828    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3201 - acc: 0.9056    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1068 - acc: 0.9683    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0749 - acc: 0.9768    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0581 - acc: 0.9824    \n",
      "Epoch 1/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.3156 - acc: 0.9068    \n",
      "Epoch 2/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.1052 - acc: 0.9686    \n",
      "Epoch 3/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0738 - acc: 0.9778    \n",
      "Epoch 4/4\n",
      "54210/54210 [==============================] - 16s - loss: 0.0585 - acc: 0.9816    \n"
     ]
    }
   ],
   "source": [
    "clustered_batch_hises = []\n",
    "for perm_num in range(20):\n",
    "    with tf.device('/gpu:1'):\n",
    "        clustered_model_his,clutered_batch_his = \\\n",
    "            train_model_large(model,clustered_rnd_xs[perm_num][:datasize],clustered_rnd_ys[perm_num][:datasize],\\\n",
    "                        epochs=epochs,batch_size=batch_size)\n",
    "        clustered_batch_hises.append(clutered_batch_his)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54210, 28, 28, 1)"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_clustered_rnd_xs[8].shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(clustered_rnd_xs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8FPX9x/HXJxcBwhFO5RIQREGrIkWq4oVV8ED70/5E\nbT2q0ha0nq14VMHWWu9qq/2pVVvBqlRbRUXRelSLgqKoiMgpyk24Azl39/P7YzaQQAIBEmZn834+\nHjxmd3Yy89lh897Jd2a+X3N3REQkvWSEXYCIiNQ9hbuISBpSuIuIpCGFu4hIGlK4i4ikIYW7iEga\nUriLiKQhhbuISBpSuIuIpKGssDbcpk0b79q1a1ibFxGJpI8//niVu7fd0XKhhXvXrl2ZNm1aWJsX\nEYkkM/umNsupWUZEJA0p3EVE0pDCXUQkDSncRUTSkMJdRCQNKdxFRNKQwl1EJA1FL9z/+1+46SaI\nxcKuREQkZUUv3F9/HW67DTZtCrsSEZGUFb1wf/bZYLp4cbh1iIiksOiF+8KFwXTGjFDLEBFJZdEL\nd/dgOmtWuHWIiKSw6IV7PB5M584Ntw4RkRQWvXBPJILpnDnh1iEiksKiF+4Vli4NuwIRkZQV3XBf\nvz7sCkREUlZ0w72kJOwKRERSVnTDvaLtXUREthHdcBcRkRop3EVE0pDCXUQkDUUv3LOywq5ARCTl\nRS/cmzQJuwIRkZQXuXBfnNEJAA+5DhGRVBa5cC9osk/YJYiIpLzIhfv4jYMBSGAhVyIikroiF+7v\nJI4FoJzscAsREUlhkQv3RXSmnCxiZIZdiohIyopcuC/f1Iyv6aojdxGR7YhcuMc9i17MZSN5YZci\nIpKyIhfuFYrQ9e4iIjWpVbib2WAzm21m88xsVDWvdzGzt81supl9bmYn132pVRUr3EVEarTDcDez\nTOBBYAjQGzjHzHpvtdhNwHh3PxQYBjxU14VurZjG9b0JEZHIqs2Re39gnrsvcPcy4Bng9K2WcaB5\n8nELoN7HwFO4i4jUrDa9cHUEFlV6vhg4fKtlRgOvm9nlQFPghDqpbjtKyAV3MN3MJCKytbo6oXoO\n8Fd37wScDIw1s23WbWbDzWyamU0rKCjYrQ2W0QjKy3drHSIi6ao24b4E6FzpeafkvMouBsYDuPsH\nQC7QZusVufsj7t7P3fu1bdt21ypOKiUHCgt3ax0iIumqNuH+EdDTzLqZWQ7BCdMJWy3zLTAIwMwO\nIAj33Ts0r0FFK0wpubB4cX1sQkQk8nYY7u4eAy4DJgGzCK6KmWlmt5rZ0ORi1wCXmtlnwNPAhe5e\nL73yZidvTC0jBz7/vD42ISISebUa1sjdJwITt5p3c6XHXwJH1m1p1WvSBMrKks0ys2fviU2KiERO\n5O5QbdUqmJbSSOEuIlKDyIV7ly7BtJRcmD8/3GJERFJU5MK9Tx8AD25i0glVEZFqRS7cDzsMsogF\n4b5xY9jliIikpMiFe58+kEks6BWytDTsckREUlLkwr17d8giHhy5JxJhlyMikpIiF+7NmkFGRbiL\niEi1IhfuOTmQUXFCVUREqhW5cA+6H3CNxCQish2RC3cAQ/25i4hsTyTDPYFRTGPqpfMaEZE0ENFw\nz9CRu4jIdkQy3MvJVpu7iMh2RDbc1SwjIlKzSIZ7RbNMvHY9FouINDiRDHcwimhCjMywCxERSUmR\nDfcY2cGAHSIiso2IhntgE03DLkFEJCVFOtyLdcWMiEi1Ih3uRTSFWCzsMkREUk6kw72YRlBQEHYZ\nIiIpJ9LhXkouzJoVdhkiIikn4uGeA598EnYZIiIpJ9LhXkYuzJgRdhkiIikn0uFeTjbMmRN2GSIi\nKSeS4Z6ZvDG1lEaweHG4xYiIpKBIhntubjAtpjGsWxduMSIiKSiS4d68eTAtogkUF4dbjIhICopk\nuHfoEEyLaQzxeLjFiIikoEiG+wEHBFONxiQiUr1IhvuBB4KR0GhMIiI1iGS49+kDGSR05C4iUoNI\nhvtBB0EmcXX5KyJSg0iGe4cOwZF7Ic00jqqISDVqFe5mNtjMZpvZPDMbVcMy/2tmX5rZTDP7e92W\nWVV2NhjOBpor3EVEqrHDEabNLBN4EPg+sBj4yMwmuPuXlZbpCVwPHOnua82sXX0VXMGBDTSnnGwa\n1ffGREQipjZH7v2Bee6+wN3LgGeA07da5lLgQXdfC+DuK+u2zG0lyGADzSnTOKoiItuoTbh3BBZV\ner44Oa+y/YD9zGyymU0xs8F1VWBNPBnu5WTX96ZERCJnh80yO7GensCxQCfgXTM7yN2rdPxiZsOB\n4QBdunTZrQ0mMDbQnFidvQURkfRRmyP3JUDnSs87JedVthiY4O7l7v41MIcg7Ktw90fcvZ+792vb\ntu2u1gxAnEwKaUaMzN1aj4hIOqpNuH8E9DSzbmaWAwwDJmy1zAsER+2YWRuCZpoFdVhnNYxycnQj\nk4hINXYY7u4eAy4DJgGzgPHuPtPMbjWzocnFJgGrzexL4G3gl+6+ur6KDhgAG2lWv5sREYmgWjVY\nu/tEYOJW826u9NiBq5P/9qiN5IE7mO3pTYuIpKxI3qFa2SaaQlFR2GWIiKSUyId7EU1gxYqwyxAR\nSSlpEO5NYUE9n7sVEYmYyIZ7RrLyIhrDzJnhFiMikmIiG+6Nk1dAFtEEPv443GJERFJMZMO9detg\nWkwTmDYt3GJERFJMZMO9WzfIIM4GmsPChWGXIyKSUiIb7occAo0oDcK9uDjsckREUkpkw71/f8ii\nnA00JxF2MSIiKSay4d67dzDU3npaoHtTRUSqimy4d+oU9C6znL3CLkVEJOVENtxbt4YYmSyis8ZR\nFRHZSmRHujALrnHfRB6l5KjjXxGRSiJ75A6QIBMngzW0CrsUEZGUEulwr6BwFxGpKuLhHlwns54W\nsGZNyLWIiKSOiId7YAMt4fHHwy5DRCRlpEm458GTT4ZdhohIykiLcN9IM5gzJ+wyRERSRqTDvaJP\n9w20gNLScIsREUkhkQ73/HwAZy35YZciIpJSIh3uhxwCOZTyBQeGXYqISEqJdLj/+MfgGF+xv7og\nEBGpJNLhfuqpUE4OC+hOPNpvRUSkTkU6ESuG2iujEQvoHm4xIiIpJNLhHgjuUl1MJ0ho2A4REUiL\ncA+spC1Mnhx2GSIiKSFtwn01beCxx8IuQ0QkJaRJuDsFtINXXgm7EBGRlBD5cG/cGHIpYTl74atW\nhV2OiEhKiHy4H3ooGM4y9iauobJFRIA0CPcbb4RiGrOIzhTSAoqLwy5JRCR0kQ/3wYMBjEV0JpM4\nvP122CWJiIQu8uFe0TPkKtpQRBMYOTLcgkREUkCtwt3MBpvZbDObZ2ajtrPcmWbmZtav7kqsLeM1\nBpNYuHDPb1pEJMXsMNzNLBN4EBgC9AbOMbPe1SzXDLgCmFrXRdaOM4sDdEpVRITaHbn3B+a5+wJ3\nLwOeAU6vZrnfAHcAJXVYX63k5QXT6RxKQvEuIlKrcO8ILKr0fHFy3mZm1hfo7O7bvYvIzIab2TQz\nm1ZQULDTxdbkuOOAZNe/xTSus/WKiETVbp9QNbMM4F7gmh0t6+6PuHs/d+/Xtm3b3d30ZtddF0yX\n0gHHYPr0Olu3iEgU1SbclwCdKz3vlJxXoRlwIPCOmS0EBgAT9uRJ1YMPDqZxsviAI0hc9JM9tWkR\nkZRUm3D/COhpZt3MLAcYBkyoeNHd17t7G3fv6u5dgSnAUHefVi8VV6Np02BqJHiPgSQ++3RPbVpE\nJCXtMNzdPQZcBkwCZgHj3X2mmd1qZkPru8DasOQ51BxK+YDvsZq6a/IREYmirNos5O4TgYlbzbu5\nhmWP3f2ydl737rBgQS7vcjSFNKN9SQnk5oZRiohI6CJ/h2qFMWMAjHJymM++8OSTYZckIhKatAn3\n007b8vgj+rPx59eGV4yISMjSJtxbtAimbVjJRE7m08SB4RYkIhKitAl3gI4doZxsPqQ/i+hC6cby\nsEsSEQlFWoX7U0/BevKJk8V7DOSDX/0j7JJEREKRVuE+cOCWx39mJJ/++f3wihERCVFahXtGRnDN\n+wj+CMBofkPpX58OuSoRkT0vrcId4JhjoIwchvE0G2nG7ItuC7skEZE9Lu3C/fbb4UWGMoxniJPF\n2YyneF1p2GWJiOxRaRfufftCAXuxF0vJZw1f0Zuf9vh32GWJiOxRaRfuOTlw3nnGuTzNStqxP7N4\nf3VP9QIsIg1K2oU7wN13wwJ6MJbzuIRHmc9+vHqhTqyKSMORluG+117B9M+M4ATeBMA+n86mdbqp\nSUQahrQMd4B99w36mNmHhQDcwJ38bfC4cIsSEdlD0jbcH30UwHiaczgtObaIT51CuQ7eRaQBSNtw\nP/ZYaNQIruReLuIx2rKSa/gD99wTdmUiIvUvbcPdDMaNgzKacD9X8CKnU0pjbrg+QXFx2NWJiNSv\ntA13gOOPD6bvcxSH8Cn7MRsng3POCbcuEZH6ltbh3qoVDB0K5eQwi168wzGA8+KLEI+HXZ2ISP1J\n63AHGDkymN7JdezNCg7mMwDOOCPEokRE6lnah/ugQcH0n5xJKTlcluwx8uWX4Yc/BPcQixMRqSdp\nH+6ZmfDVV0HTzOt8n0t4nKYUAvD8c3HeeivkAkVE6kHahztAr15www0wnIcpJ4uJnAKAk8nIS0t0\n9C4iaadBhDvAbbdBXo+OXM4fGch7HELQk9jsr3PJy4NYLOQCRUTqUIMJd4Df/Q4e5mdk4EylPyP5\nEzmUUlQE/fuHXZ2ISN1pUOF+1llw/vnB46YUcS9XU0RjMokxfTrstx/qnkBE0kKDCnczuPnmYKzV\nGNkM5D9k4PyBKwCYOxdatoTHHgu5UBGR3dSgwh2C3iKvvz54/CEDKKAtl/EQ/8NzABQVwSWXwDPP\nhFikiMhuanDhDjB6NJx+OoBxBXcD8Dw/5LfcsHmZc86Bjh1hzZpQShQR2S0NMtyzsuCFFyA/H/7B\nudzDlQDcyO0M58+bl1u6FE47TTc6iUj0NMhwr/DGGxAni3u4ljMZD8DDjGAmB2xe5v334cgjYdWq\nsKoUEdl5DTrcDzsMnn4altGRCZzGXzmfGBn05ivuSx7NA3zwAbRtC3fcEWKxIiI7oUGHO8DZZ8Pe\ne0OMXG5iDLkUsZ5mXMH9nMhEMohzOB9wM6O5YVScYcNcR/EikvJqFe5mNtjMZpvZPDMbVc3rV5vZ\nl2b2uZm9aWb71H2p9cMMFi+Gu++GJXQlTiPu4yoMmMQpnM+TTOV73MpoHuUSnn3W6NbN1Q4vIilt\nh+FuZpnAg8AQoDdwjpn13mqx6UA/d/8O8BxwZ10XWp8yMuCqq4I+aAB+z6/4A5cDcAO/w0gAcDFP\nsI4WtNi4hO/2XKc+4UUkZdXmyL0/MM/dF7h7GfAMcHrlBdz9bXcvSj6dAnSq2zLrX0ZGsvfIcrDc\nplzFPdzCLXxMXzqyiB58BUBL1nM8b7J6/lqysuCEE+C9d12X1IhISsmqxTIdgUWVni8GDt/O8hcD\nr+5OUWHKygpuZOq5j3ProtFVXmvFStbQjrFcwHOcyVscz0NvjuSTN9fyZsaJHLrgedgnMi1SIpLG\n6vSEqpn9COgH3FXD68PNbJqZTSsoKKjLTdcpM5g6PWeb+Wtox6+4DYCzeJ5R/B7HWENrOia+pajr\n/sEP/+hHsGHDni5bRGSz2oT7EqBzpeedkvOqMLMTgBuBoe5eWt2K3P0Rd+/n7v3atm27K/XuMa1b\nw7p1286/kxvozEIAurCIb+gCQDsKaEJJsNBTT0GHDrBx4x6qVkSkqtqE+0dATzPrZmY5wDBgQuUF\nzOxQ4GGCYF9Z92WGo0ULWLs2aI/fwlhEV7IIvr+68g0jeYBPObjqD2/aFFxjKSISgh2Gu7vHgMuA\nScAsYLy7zzSzW81saHKxu4A84B9m9qmZTahhdZHTsiWUlMARR1SdH6PR5scPcTmH8SGn8BK/4g7m\nsS8AKzc2hjff3JPliogAYB7SVR79+vXzadOmhbLtXRGPQ/fu8O23tVnaeZCRjODP/JI76co8RtzU\nHvvNrfVdpoikOTP72N377Wi5Bn+Ham1lZsK119Z2aeNa7uIX3E8WMY7gQ2b99jliZnDlldCtWzAt\nKanPkkWkAdOR+05wh3vv3ZmQ3+I9juIoJhMHMivN/3Zhgg4djazaXJQqIg2ejtzrgRlcc01wFc1N\nN8Hf/w7Tp0PTpjX9hNOLr8ihlIH8l/H8kPc5ist5gLu5hnl0556ud5CdDdddB4WFe/LdiEg605F7\nHVm2DB59FG6/fedaW85lLM9yNnFy6M4cRrUfS+6vr+Wcn7bYfDRfVBSs88knYciQLd0kiEjDU9sj\nd4V7PYjHYf36oDOy22+v/IoDVs1j6MUsJnMkrVkbrIMMJnAq/8OL26w/m1J+zRi6HdONox/5MYXl\nufTpU1/vRkRSiZplQpSZCa1awW23BV0Kb2FVHjdmE8fyNi1Yx2wO4BQmUkhesA4S/IAJnMa/yKYs\nOa+cBxhBGbn8mtv50X+G06VXYx488E9s2rTH3p6IRIDCvR6ZwbhxcP/90LXrtq8X05R3OI6WrKM7\n85nKAJpTyJk8hwPLac8hzOBsnuUlTuEFTmcTzVlH8yrreYjLaZpnwcCwd90FZ5wBjRoFI5GISIOk\nZpk96JVXgsG53eHjj7fMz6aMBBBnS382GcRoQjEbabZ53veYzAaaM5OD+AX3cwfXkUu1PT1stuLD\nb2j/3S51/E5EJCxqc09xhYXQseOOr5B5l4G8wQncwSjKKt0VW+GPXMY/+R+yiLGMvRnBQ/yUh8kg\n+H91YAXt2HTqOSwd8Vs69sqje/d6eEMiskco3CNgzZqgb7EuXWDhQkgkgpOxPXoEl0Y++qhTtq6Q\nIpoBzuF8SCZxPuK7lLNtr5UVOrCE/3AMPZhPnAwySeBAjCyO420mcxQQXH0zaFDQBY5ZjasTkRSi\ncE8jkyfDSy/BD34AOTlw4onUMI6r05ElLKk0VkouxfyUh/k9o5hLTw7mM3zzqZYtV+yYwT33wIQJ\nwUDgXbtCs2YwY0YwkHhm5jYbE5EQKNzT3M9/Dv/3f7u/nk4s4mRe4R2OZQ77b3fZZs2CZqTOneFn\nP4Mzz4RHHoEpU+CYY4IvnBEj4JBDguvyN20Kbvhq3x7y8na/VhFRuDcI8+cHNzjl5cHzz8OoUexw\nXNenOJdzeZoFdOU2buJxLq7zutq2DUa0WrZsy7xXJ5Rz0qnZTJoUDEh+9tlB3bEYZGfXeQkiaUvX\nuTcA++4LBx0U9EN27bVBu33FGCiZmVv3Qx/4MWM5gsm0ZB2PcQmF5PFL7uREJjGO8yihEY7xFb1Y\nQz73cmWtamnNKowYABkFyzh82b/4EyNxjA/5Lu8PvZ2TMl7nrCEbGXXpKvZvvpi/ZFzKf3KOZ1NG\nHgweDEu2GQNGRHaRjtzTmDs8+CD89rewYkW1S1R5dj5/5QZu41EupROL+Cl/oXHyUstxnMctjOE0\nXuIC/sbrnEgHlvIypzCeYVXW8wP+yeX8kR7MowNLiZFFo+SNWDv0wAPQoweJk4Zs+XLasCFoE0qe\n9Y3Fgi+u6r68RNKdmmVks5ISuPxy6N0bFiyAgQODK3R+9jP47LPg6H/AgKBvnMpO5BVe5bTNl1Vu\nz0SGMJxHqpzMrawdKxjHebzCqYzgIfZjLqtpxWccTD8+ojkbeZeB5FHI3iynPcvZQB5N87LJ3hh0\nyUCbNix8Zgr9z9mXggK49hdl/G7m6WSXFAad7V9xRXD2l6B56h//CO7lOukkWL48OFegJiCJOoW7\n7LRVq4Kj/U8+CZp5vvgCGlNERxZTTjbFNGEVbWjKJjJI0Jz15LOO5bTnPq5mGM/wLZ25kd8ynmHE\n2F6SOjmUJq/d3/Y6zAt5gocYQSZxCmhLR5YCsIlc7uaXXM/tyXVUPclwHk/Shy8B43F+wnx60J15\n/I7rWUtLXsk6k9Znn8DjT2ZRWgqNG9fZ7hPZI2ob7rh7KP8OO+wwl9SVSLjH4+5PPeV+yinuc+e6\nDxrkHsR//fzLIOZ5rPcMYtW+fhgfObg3pdAv536PkVHtihKVHl/D76s8d/AZ7O8Dedvbs9jP++4s\n/7TvhR7PaeSxa37pI0a4H320+89/7v7ZZ+5lZdvum7fecs/NdT/jDPdNm9xjsWD+0qXuf/qT+4oV\n7osXB/tQpK4B07wWGasjd9kp69bB0qVw331QVgbl5Vu6sGnePGge330Vn0mj8rX4jSihlNzNSw3i\nDRpRQiZx+vIJw/kLjSlmDvuxL/PJpQTH+CkPs57mDOQ9ruNO4mTyMqfyKkPIYyNXcR+dCE7mrieP\n+fSgNau5l6t4gKvIzU1QUpLBfffB2LHBXza10bw5XHUVvPde0CR0ySXBVURt2wYnw7exfHnQL5AZ\n9O8PF10UXFcqUomaZSQ0iUQQ/G+8AUOHVn2tf39o1w4mTgyW2xX5rKaQ5ts0+2QQJ8Gu3W11A7ex\nF8tZRGd6MI8j+S8P8XMeZTgJMohX2lYTCmlKEWtoVWV+9ZzmbKAVq1lPC9bSCjD2bhcjt2kWl1wC\nV357NZMfnsF13MERTGYpHbiF0RzMF5R06E5Ov++Q8eUXwa3L774b7NwePeCf/4QDDtil9yvRpWYZ\nSQmLF7tv2OBeXOxeVLRlfkmJ+/jx7qtWBc0/7u7LlrkPHereuPGWVpTMTPeePd2bNq3a+pKbG7zW\nrFnV+ZmUehM2uBH3Jmz0bsxziFfbzJPHBt+XuTtsLhrEG96eZZ5F2Tav5VLkJ/OSf5epfiTveS5F\n1awj4RmUOySqXf9BfOa3cb2Xk7l55jqa+0wO8PN5wq/hTv+Aw72EnG1+eG1OG0+Axy/8iXtJicfj\n7queed192DBPzJ3n7u6vveb+8su1+/+qaErauNG9tLQOPwhSZ6hls4zCXVJWScmW9mz3IHDKy7dd\nbvZs90sucX/ooeB5LOZeUOA+fHjQdj5ggHv37u4Z1TfRJ78UyvwQPvah/MuP5N1kGLvfwi0+i/3c\nweOYv8aJfhgf+Zn8wx/l4mrb/Rewj1/G/d6Jbx3ch/BKtdscxlPelQV+NG/7EF7xX/F7X0AXd/Ai\ncqstdCFd/Bru8s84yD+kn7/AaV6W/FKIYV5CVpXlZ9LL81nlzVnjP+YxLyJn83L3MdJ7d1rrnfI3\nJr94gi+fphmb/HLu8nc40nvxheflud9yi/v69ZXOI8yb57E16/3aa4NNXX21++TJwfmIii/ryhYu\n3PJlsW5dsEzFF3xxsfuiRXX5yUlvtQ13NctIgxGPB90l3H8/dOoEHToEd/YOGwZ9+gSjZ732WnDn\nbGZmcJnoypVB98t9Mz8nFjda2AbyfS2L6MihfMoaWpJBghc4g5/wGH/msmovHV1DS5pTyJscz2jG\nMIXvbafS4DxDC9aynnz68AVryWcpHajuyiKAg/mU03mRfZnPSB5kIO8xmNeIk8kK2nM199KOghq3\nGCOTMnL4Py7lKh6ospU4Rgm5fMRhZBHnCD7YfPfjvVzBVAbQjpWM48esI5/27YPbEvr3DwatGTcu\nOFcDwX6Nx4N7FLZulmvRAo48Mnht6FA4+WRo0gTWr4kz+T8xBp3ciKZNg3Uzfz4+6XXsr0/gZ/wA\nu+H67ezP9KI2d5E6UFwchNH2+sZZsiT4wpj0SoyVX67kN/yaDxjAexzNXKof8PYY3qaEXDbQnFn0\npqbQ3rHKJ59r1oJ1PMylbCKP9zmC03mRw5nCeIbxHGfxMYdxEDPIpYS3OR6APsygO1+zgvZ8yOGb\n19WTOeRSwrG8wxAmMoApLKET7zGQXzOG1bTdqp4EzSikkBab52RRTgbxGi+FNeIYVHsOpQPfcjhT\nacMa/sgv+JpufEEfTuZVcru0Z/rwh/hk7Az6LXyeA0s/Jos4jkF2NiXX3UKTfTvAhRdWKi8RnMSu\nrmvUsrJgfjU3SLiH05uqwl0kBCtWBB26XX55cFXR/PnB0WsiAX/7GzzxRHBCeeXK3d9WxVHwjm25\n4qg6RgIjQdbmsK1mW8TIYyMJjE3kbRO6LVjHelqSTVmV7qhzKaYXszmPcWQS52Z+wya234tca1aR\nICN58nnH2rKSs3mWU3mZXEooojHf0JUldGQuPWnNak7lZT7ge7zCKXRgKddyN535hjhZTKMfffmE\nr9ifGfThDruRjZ7H0KyX+XXsFpqxgW7M50sO4ozsiXTIXUtJUYJFiY6cdG5rxo7dsyGvcBdJYe5B\np28A48fDN98El0u++GLQvXP//kF4L1gAt94KZ50Fs2cHPzNoUPBlkZER3H386adBk9LUqcFdyN/5\nDjz0ELzwQvAFU15eddtZWUEXDgA9W61iZXEzCkuycXdyKaKYPKoO5A7b/8tgy5fH/sziaP5DK9bw\nEkPZRFMW0q3K0hfwV07lJXoxm4OYySccygrakyCD7ixgf76qsvVl7M3XdGMqh+MYK2kHOEU04R2O\no4gmLKC6a0u3L5MYWcSqXF7bitUU05jjeYvfcBOtWY0BnVmMA+VkkUOMOEY5OQzkLTYdcATTpgVN\nSHuCwl1EgODo/quvgkFZ8vO3HGXG4zX30z9zZvBF873vBW3hEHT78/rrsHZt8KViFnxJVS9BLiV8\nN/MTinJb8mVZLw4qn8aPGcdeLOUWxrCRZjzFeRzFBwB8SyfeZBAFtGUOvZjOoezFEpxM2lLAMfyH\n43ib6RzK9fyeJXRgU6VhKAGyKKMbX1NEY9bRMvlXQgaQADIw4jiZGAl6MpembGQ5e5NJnC58S3PW\n8xH9KaRZlb9izmMcA5jCqwxhBe3JoYzv8Dk9mc1TnM2MrAG8+iqccMKu/i/VnsJdROpNRXvz9OlB\n+FcM3fjtt0HvpEcfve3PFBcHJ1bz84O/Hr76CvKnTOTBy2dzxt5TWX/YMfy7w0V03T+X444Lxg6Y\nOTPoI+jdd4NRyyD4i6VRo2B9u/EOqOmvkSzKyWcNBbQHEmTgm5uh8iikCUUU05gYWTjGFfyBPDZw\nP1dx/1PtOPfc3alrxxTuIpJWCguDJqZWyab4BQuCHk+feAKOPRb23x++//2g9+hJk+DVV4OBYq6/\nfkuTSXFqjPRQAAAGN0lEQVRxMDj9hx8G50b22w+OOy5o0po1K2je2rAh+OLKz4fcXCgthYKCYB2t\ny5bQt8NyFjfrzc+n/IgxjGE+PbiK++jPB0yjL01vG80NN9TfflC4i4jUJ3fe/M6V3PXFYCYxhMOZ\nwtXcw/OcxgG3nM/o0fWzWQ3WISJSn8wYNON+/vfuAZzN35lJHy7mcXrwLU+MmcuTT4ZbnsJdRGQ3\n/OSafC567VzyWU135vM7bmIv1jDugomMHRteXQp3EZHddNJJMH1VVzplLeMmxvAxh7GGtkw9/wHG\njAmnJoW7iEgdaN0aXikfQseRZzCGm5nN/vyF4cwZPY7/PWLBHq+nVuFuZoPNbLaZzTOzUdW83sjM\nnk2+PtXMutZ1oSIiUfCzPx3MBW9dwB8ZwYF8wd/5EWUffMZYG7bL3Vzvih2Gu5llAg8CQ4DewDlm\n1nurxS4G1rp7D+A+4I66LlREJCo6HdeLCxJjOZmXGMkfeZEf8BQX0jfzw1p2GbH7anPk3h+Y5+4L\n3L0MeAY4fatlTgf+lnz8HDDILIwudUREUoMZ3OpjKDjrZ1zEX/g3J7CcLpya9fJ27uytO7UJ947A\nokrPFyfnVbuMu8eA9UDruihQRCTKnv1HNjfOOI8x3MjeLOM1TuWKjD/U+3az6n0LlZjZcGA4QJcu\nXfbkpkVEQrPvgY250e+gsV1BDqWU7YHorc0WlgCdKz3vlJxX3TKLzSwLaAGs3npF7v4I8AgEd6ju\nSsEiIlF1td/P1QAMqPdt1aZZ5iOgp5l1M7McYBgwYatlJgAXJB+fBbzlYfVrICIiOz5yd/eYmV0G\nTAIygcfdfaaZ3Uowlt8E4DFgrJnNA9YQfAGIiEhIatXw4+4TgYlbzbu50uMS4Id1W5qIiOwq3aEq\nIpKGFO4iImlI4S4ikoYU7iIiaUjhLiKShkIbZs/MCoBvdvHH2wCr6rCcdKH9Uj3tl5pp31QvlffL\nPu7edkcLhRbuu8PMptVmDMGGRvuletovNdO+qV467Bc1y4iIpCGFu4hIGopquD8SdgEpSvuletov\nNdO+qV7k90sk29xFRGT7onrkLiIi2xG5cN/RYN3pzswWmtkMM/vUzKYl57UyszfMbG5ymp+cb2b2\nQHJffW5mfcOtvu6Y2eNmttLMvqg0b6f3g5ldkFx+rpldUN22oqSG/TLazJYkPzOfmtnJlV67Prlf\nZpvZSZXmp9XvmZl1NrO3zexLM5tpZlck56fvZ8bdI/OPoMvh+UB3IAf4DOgddl17eB8sBNpsNe9O\nYFTy8SjgjuTjk4FXASMYHWBq2PXX4X44GugLfLGr+wFoBSxITvOTj/PDfm/1sF9GA9dWs2zv5O9Q\nI6Bb8ncrMx1/z4C9gb7Jx82AOcn3n7afmagduddmsO6GqPIA5X8Dzqg0/0kPTAFamtneYRRY19z9\nXYKxAyrb2f1wEvCGu69x97XAG8Dg+q++/tSwX2pyOvCMu5e6+9fAPILfsbT7PXP3Ze7+SfJxITCL\nYOzntP3MRC3cazNYd7pz4HUz+zg5Ji1Ae3dflny8HGiffNzQ9tfO7oeGtH8uSzYvPF7R9EAD3S9m\n1hU4FJhKGn9mohbuAke5e19gCDDSzI6u/KIHfzs2+EugtB+q+DOwL3AIsAy4J9xywmNmecDzwJXu\nvqHya+n2mYlauNdmsO605u5LktOVwL8I/oReUdHckpyuTC7e0PbXzu6HBrF/3H2Fu8fdPQE8SvCZ\ngQa2X8wsmyDYn3L3fyZnp+1nJmrhXpvButOWmTU1s2YVj4ETgS+oOkD5BcCLyccTgPOTZ/4HAOsr\n/QmajnZ2P0wCTjSz/GRTxYnJeWllq/MsPyD4zECwX4aZWSMz6wb0BD4kDX/PzMwIxnqe5e73Vnop\nfT8zYZ/R3dl/BGex5xCczb8x7Hr28HvvTnDlwmfAzIr3D7QG3gTmAv8GWiXnG/Bgcl/NAPqF/R7q\ncF88TdDEUE7Q7nnxruwH4CcEJxLnAReF/b7qab+MTb7vzwlCa+9Ky9+Y3C+zgSGV5qfV7xlwFEGT\ny+fAp8l/J6fzZ0Z3qIqIpKGoNcuIiEgtKNxFRNKQwl1EJA0p3EVE0pDCXUQkDSncRUTSkMJdRCQN\nKdxFRNLQ/wOUds7lU/9bmwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c7803df90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start=0\n",
    "end  = 10000\n",
    "window = 100\n",
    "for i in range(20):\n",
    "    plt.plot(runningMeanFast(normal_batch_hises[i][start:end],window),color = \"red\", label= \"normal_loss\"+str(i))\n",
    "    plt.plot(runningMeanFast(clustered_batch_hises[i].losses[start:end],window),color=\"blue\", label = \"clustered\")\n",
    "# plt.legend()\n",
    "plt.savefig('Keras_deep_cmpGradient_bunchofnormal.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
