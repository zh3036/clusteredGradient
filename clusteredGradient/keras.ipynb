{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# some material\n",
    "   \n",
    "### Is the data shuffled during training?\n",
    "\n",
    "Yes, if the shuffle argument in model.fit is set to True (which is the default), the training data will be randomly shuffled at each epoch.\n",
    "\n",
    "Validation data is never shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten,Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# np.random.seed(2016) \n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo\n",
    "0. check whether the gradient decent is not shuffled  ---- done \n",
    "1. make the intilization the same   --- done \n",
    "2. make the datasets clustered  ----done \n",
    "3. randomized the clustered batch but its clusterng\n",
    "\n",
    "\n",
    "# the original complex keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "47616/60000 [======================>.......] - ETA: 8s - loss: 0.3761 - acc: 0.8855"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-924522fad792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m           validation_data=(x_test, y_test))\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zihanz/virtualenv/ML/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a simpler SGD model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.6508 - acc: 0.8433 - val_loss: 0.3052 - val_acc: 0.9128\n",
      "Epoch 2/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.2870 - acc: 0.9184 - val_loss: 0.2448 - val_acc: 0.9317\n",
      "Epoch 3/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.2360 - acc: 0.9328 - val_loss: 0.2077 - val_acc: 0.9390\n",
      "Epoch 4/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.2025 - acc: 0.9430 - val_loss: 0.1859 - val_acc: 0.9463\n",
      "Epoch 5/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1789 - acc: 0.9497 - val_loss: 0.1631 - val_acc: 0.9530\n",
      "Epoch 6/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1590 - acc: 0.9551 - val_loss: 0.1594 - val_acc: 0.9542\n",
      "Epoch 7/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1444 - acc: 0.9588 - val_loss: 0.1364 - val_acc: 0.9606\n",
      "Epoch 8/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1321 - acc: 0.9620 - val_loss: 0.1303 - val_acc: 0.9619\n",
      "Epoch 9/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1219 - acc: 0.9653 - val_loss: 0.1215 - val_acc: 0.9645.96 - ETA: 0s - loss: 0.1221 - acc:\n",
      "Epoch 10/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1134 - acc: 0.9671 - val_loss: 0.1228 - val_acc: 0.9642\n",
      "Epoch 11/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.1060 - acc: 0.9696 - val_loss: 0.1099 - val_acc: 0.9673\n",
      "Epoch 12/12\n",
      "60000/60000 [==============================] - 17s - loss: 0.0999 - acc: 0.9711 - val_loss: 0.1069 - val_acc: 0.9694\n",
      "Test loss: 0.106879658414\n",
      "Test accuracy: 0.9694\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "17s - loss: 0.7178 - acc: 0.8193 - val_loss: 0.3196 - val_acc: 0.9094\n",
      "Test loss: 0.319608975106\n",
      "Test accuracy: 0.9094\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 1\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KERAS SGD TESTING\n",
    "start from here we are testing the keras SGD random or not, we use small sample of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test=x_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test=y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (5000, 28, 28, 1)\n",
      "5000 train samples\n",
      "1000 test samples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "5000/5000 [==============================] - 8s - loss: 1.4065 - acc: 0.6968 - val_loss: 0.7591 - val_acc: 0.8070\n",
      "Test loss: 0.759067568779\n",
      "Test accuracy: 0.807\n",
      "[2.3049223, 2.2985256, 2.2503564, 2.23441, 2.2234449, 2.2072704, 2.1865511, 2.2155533, 2.1858051, 2.1327934, 2.1990898, 2.1380169, 2.1394863, 2.103225, 2.0474067, 2.0465572, 2.0727258, 1.9859893, 2.041285, 1.9573172, 1.9655675, 1.9812416, 1.9547147, 1.9515655, 2.0535624, 1.8622813, 1.8708495, 1.8733219, 1.8495508, 1.8177255, 1.7837791, 1.8093818, 1.7864228, 1.7551713, 1.6808974, 1.6290058, 1.7298985, 1.6548636, 1.5631753, 1.6652234, 1.4998133, 1.4984107, 1.4360192, 1.4625568, 1.363209, 1.5035322, 1.4647084, 1.3634138, 1.4724407, 1.423772, 1.321414, 1.3188597, 1.214731, 1.2739127, 1.3362553, 1.235743, 1.2167237, 1.040653, 1.1309727, 1.1397777, 1.1723866, 1.103657, 1.0912642, 0.99940914, 0.94918859, 1.0763081, 1.1367857, 1.0637504, 1.0026662, 0.79261637, 0.87819529, 0.83536613, 0.79376453, 0.86479247, 0.86266452, 0.85155499, 0.90317357, 0.88909763, 1.0159655, 0.76894772, 0.85069048, 0.82468623, 0.91659057, 0.71294498, 0.88365251, 0.78887117, 0.75858307, 0.8007558, 0.76175386, 0.66194952, 0.7351917, 0.84791619, 0.80403364, 0.69142652, 0.85679936, 0.81937689, 0.59881306, 0.73300576, 0.82875723, 0.79295349]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    # np.random.seed(2016) \n",
    "    batch_size = 50\n",
    "    epochs = 1\n",
    "\n",
    "\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['acc'])\n",
    "\n",
    "    batch_history = LossHistory()\n",
    "\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks = [batch_history])\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print(batch_history.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('keras_stochas_test.h5')\n",
    "#Assuming you have code for instantiating your model, you can then load the weights you saved into a model with the same architecture:\n",
    "\n",
    "model.load_weights('keras_stochas_test.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "5000/5000 [==============================] - 0s - loss: 0.4911 - acc: 0.8752 - val_loss: 0.4847 - val_acc: 0.8570\n",
      "Test loss: 0.484671998024\n",
      "Test accuracy: 0.857\n",
      "[0.74001825, 0.50991201, 0.67236155, 0.63878816, 0.58162868, 0.58588982, 0.49227825, 0.44537508, 0.60324645, 0.61554861, 0.81974787, 0.62652075, 0.81085724, 0.63647437, 0.58922052, 0.50366402, 0.52365553, 0.72443247, 0.74182057, 0.51571625, 0.72931361, 0.80613327, 0.7977227, 0.41920802, 0.66189563, 0.5539059, 0.45504004, 0.69632268, 0.56362122, 0.41638905, 0.54011858, 0.56462443, 0.4719522, 0.50067538, 0.33932644, 0.49572638, 0.49912605, 0.4305236, 0.46893668, 0.36471185, 0.56305152, 0.50469106, 0.29429141, 0.38880646, 0.43068427, 0.3661879, 0.409724, 0.47507751, 0.61898494, 0.3900775, 0.29899412, 0.44382507, 0.41442293, 0.52878785, 0.54420567, 0.48296213, 0.48953086, 0.29178518, 0.50160062, 0.43842167, 0.5282855, 0.58993357, 0.41710049, 0.35561889, 0.41067433, 0.49818552, 0.31359819, 0.58761752, 0.38501012, 0.37142059, 0.61493379, 0.36351019, 0.27638412, 0.55661404, 0.45471004, 0.45846593, 0.41743988, 0.32518482, 0.35878998, 0.342022, 0.28102133, 0.57798684, 0.46101761, 0.57744765, 0.32036161, 0.43766263, 0.48834184, 0.41004321, 0.30174226, 0.45654821, 0.37843686, 0.24324566, 0.47529745, 0.51853216, 0.26317567, 0.53444099, 0.4972057, 0.26363698, 0.43480033, 0.53379714]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.load_weights('keras_stochas_test.h5')\n",
    "with tf.device('/gpu:1'):\n",
    "    history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              validation_data=(x_test, y_test),\n",
    "              callbacks = [batch_history],\n",
    "              shuffle = False) # we can already see that the shuffle affects training a lot\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    print(batch_history.losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary\n",
    "here \n",
    "1. we make keras stop shuffling the data \n",
    "2. we make sure it gives a determinisc GD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# make data clustered\n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load and formating data first \n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "num_classes = 10\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([30207,  5662, 55366, ..., 23285, 15728, 11924])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_X_train = x_train[y_train.argsort()]\n",
    "sorted_Y_train = y_train[y_train.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(sorted_Y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5923, 6742, 5958, 6131, 5842, 5421, 5918, 6265, 5851, 5949])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5421\n"
     ]
    }
   ],
   "source": [
    "num_each = min(counts)\n",
    "print (num_each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first we want a list of indices of each number \n",
    "\n",
    "# array of indices of 0\n",
    "\n",
    "sorted_Y_train[range(0,counts[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind1 = np.argwhere(sorted_Y_train  == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5923],\n",
       "       [ 5924],\n",
       "       [ 5925],\n",
       "       ..., \n",
       "       [12662],\n",
       "       [12663],\n",
       "       [12664]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5923,  5924,  5925, ..., 12662, 12663, 12664])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind1.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listOfIndOfNum = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listOfIndOfNum = [np.argwhere(sorted_Y_train ==i).flatten() for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ..., 0 0 0]\n",
      "[1 1 1 ..., 1 1 1]\n",
      "[2 2 2 ..., 2 2 2]\n",
      "[3 3 3 ..., 3 3 3]\n",
      "[4 4 4 ..., 4 4 4]\n",
      "[5 5 5 ..., 5 5 5]\n",
      "[6 6 6 ..., 6 6 6]\n",
      "[7 7 7 ..., 7 7 7]\n",
      "[8 8 8 ..., 8 8 8]\n",
      "[9 9 9 ..., 9 9 9]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(sorted_Y_train[listOfIndOfNum[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# now let us make a function that can join them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(partitions_inds):\n",
    "    cluster_size = len(partitions_inds)\n",
    "    print (\"cluster size:\",cluster_size)\n",
    "    cluster_num = min([len(i) for i in partitions_inds])\n",
    "    print (\"cluster_num:\",cluster_num)\n",
    "    ret = np.arange(cluster_size * cluster_num)\n",
    "#     print (ret)\n",
    "    print (ret.shape)\n",
    "    for i in range(cluster_num):\n",
    "        for j in range(cluster_size):\n",
    "            pass\n",
    "            ret[cluster_size*i+j] = partitions_inds[j][i]\n",
    "#             print (j,i)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster size: 10\n",
      "cluster_num: 5421\n",
      "(54210,)\n"
     ]
    }
   ],
   "source": [
    "mixed_ind = join(listOfIndOfNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(threshold=100)\n",
    "sorted_Y_train[mixed_ind].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_Y_train = sorted_Y_train[mixed_ind]\n",
    "clustered_X_train = sorted_X_train[mixed_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summary\n",
    "I have made the indices that are well clustered\n",
    "\n",
    "notice that the training data here is already formated and can be directly used for model.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare cluster with fixed or shuffled\n",
    "\n",
    "it may be useful \n",
    "\n",
    "np.random.permutation(10)\n",
    "\n",
    "array([1, 7, 4, 3, 0, 9, 2, 5, 8, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 2, 9, 7, 5, 0, 4, 1, 3, 6])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.permutation(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clustered_x_train shape: (54210, 28, 28, 1)\n",
      "54210 train samples\n"
     ]
    }
   ],
   "source": [
    "# let us first make Y to be one hot \n",
    "num_classes = 10\n",
    "clustered_y_train = keras.utils.to_categorical(clustered_Y_train, num_classes)\n",
    "clustered_x_train = clustered_X_train\n",
    "\n",
    "# let us normalize a bit the intilization value\n",
    "\n",
    "clustered_x_train = clustered_x_train.astype('float32')\n",
    "clustered_x_train /= 255\n",
    "\n",
    "print('clustered_x_train shape:', clustered_x_train.shape)\n",
    "print(clustered_x_train.shape[0], 'train samples')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let make several random shuffled dataset\n",
    "rnd_inds = [np.random.permutation(clustered_x_train.shape[0]) for i in range(20)]\n",
    "rnd_xs = [clustered_x_train[ind] for ind in rnd_inds]\n",
    "rnd_ys = [clustered_y_train[ind] for ind in rnd_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can load our model here to ensure the same inilization\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        \n",
    "def makeModel():\n",
    "    # np.random.seed(2016) \n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False),\n",
    "                  metrics=['acc'])\n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x7fcfd9b5fe50>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "makeModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model,x_train,y_train,batch_size=50,epochs=1):\n",
    "    batch_history = LossHistory()\n",
    "    model.load_weights('keras_stochas_test.h5')\n",
    "    model_history = model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              verbose=1,\n",
    "              shuffle = False,\n",
    "              callbacks = [batch_history])\n",
    "#     print(batch_history.losses)\n",
    "    return model_history,batch_history\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = makeModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasize = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "54210/54210 [==============================] - 150s - loss: 0.2357 - acc: 0.9309   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    clustered_model_his,clustered_batch_his = \\\n",
    "        train_model(model,clustered_x_train[:datasize],clustered_y_train[:datasize],\\\n",
    "                    epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "54210/54210 [==============================] - 148s - loss: 0.2414 - acc: 0.9286   \n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.device('/gpu:1'):\n",
    "    normal_model_his,normal_batch_his = \\\n",
    "        train_model(model,rnd_xs[2][:datasize],rnd_ys[2][:datasize],\\\n",
    "                    epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcfd3630690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "start=0\n",
    "end  = 10000\n",
    "# plt.plot(loss_list_clus[start:end],label = \"clus_loss\")\n",
    "plt.plot(clustered_batch_his.losses[start:end], label = \"clus_loss\")\n",
    "# plt.plot(loss_list_normal[start:end], label = \"normal_loss\")\n",
    "plt.plot(normal_batch_his.losses[start:end], label= \"normal_loss\")\n",
    "plt.legend()\n",
    "plt.savefig('Keras_deep_cmpGradient_epo'+str(epochs)+'_batsiz'+str(batch_size)+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
